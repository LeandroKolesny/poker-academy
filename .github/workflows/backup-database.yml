name: Backup Database (Diário)

# Executa diariamente às 06:00 UTC (03:00 BRT)
# Também pode ser executado manualmente pelo GitHub
on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:

jobs:
  backup-database:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Instalar cliente PostgreSQL
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y -qq postgresql-client

      - name: Testar conexao com o banco
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          # Extrair hostname da URL para testar DNS
          HOST=$(echo "$DATABASE_URL" | sed -E 's|.*@([^:]+):.*|\1|')
          echo "Testando DNS para: $HOST"
          nslookup "$HOST" || echo "DNS falhou, tentando dig..."
          dig +short "$HOST" || echo "dig tambem falhou"
          echo "Testando conexao com psql..."
          psql "$DATABASE_URL" -c "SELECT 1 AS connection_ok;" || echo "Conexao falhou"

      - name: Criar backup do banco de dados
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          FILENAME="backup_${TIMESTAMP}.sql"
          FILENAME_GZ="${FILENAME}.gz"

          echo "Iniciando backup do banco de dados..."
          echo "Porta na URL:"
          echo "$DATABASE_URL" | sed -E 's|.*@[^:]+:([0-9]+)/.*|\1|'

          # Primeira tentativa: pg_dump normal
          echo "=== Tentativa 1: pg_dump direto ==="
          pg_dump "$DATABASE_URL" --no-owner --no-privileges -f "$FILENAME" 2>pg_dump_error.log
          PG_EXIT=$?
          echo "Exit code: $PG_EXIT"
          if [ -s pg_dump_error.log ]; then
            echo "Erros do pg_dump:"
            cat pg_dump_error.log
          fi

          # Se falhou, tentar com psql (funciona com qualquer pooler)
          if [ "$PG_EXIT" -ne 0 ] || [ ! -s "$FILENAME" ]; then
            echo ""
            echo "=== Tentativa 2: psql --command com schema dump ==="
            psql "$DATABASE_URL" -c "\dt" 2>psql_error.log || true
            if [ -s psql_error.log ]; then
              echo "Erros psql dt:"
              cat psql_error.log
            fi

            echo ""
            echo "=== Tentativa 3: pg_dump com --no-synchronized-snapshots ==="
            pg_dump "$DATABASE_URL" --no-owner --no-privileges --no-synchronized-snapshots -f "$FILENAME" 2>pg_dump_error2.log
            PG_EXIT=$?
            echo "Exit code: $PG_EXIT"
            if [ -s pg_dump_error2.log ]; then
              echo "Erros:"
              cat pg_dump_error2.log
            fi
          fi

          # Se ainda falhou, tentar export via psql COPY
          if [ "$PG_EXIT" -ne 0 ] || [ ! -s "$FILENAME" ]; then
            echo ""
            echo "=== Tentativa 4: backup via psql custom queries ==="
            psql "$DATABASE_URL" <<'SQLEOF' > "$FILENAME" 2>psql_backup_error.log
-- Schema
SELECT '-- Backup gerado em ' || now();
-- Listar todas as tabelas
SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' ORDER BY table_name;
SQLEOF
            echo "psql exit: $?"
            if [ -s psql_backup_error.log ]; then
              echo "Erros:"
              cat psql_backup_error.log
            fi
          fi

          # Verificar resultado
          if [ -s "$FILENAME" ]; then
            gzip "$FILENAME"
            FILE_SIZE=$(du -h "$FILENAME_GZ" | cut -f1)
            FILE_BYTES=$(stat --format=%s "$FILENAME_GZ")
            echo ""
            echo "Backup criado: $FILENAME_GZ ($FILE_SIZE, $FILE_BYTES bytes)"
            echo "BACKUP_FILE=$FILENAME_GZ" >> $GITHUB_ENV
          else
            echo "ERRO: Todas as tentativas falharam."
            exit 1
          fi

      - name: Upload backup para Cloudflare R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          aws s3 cp "${{ env.BACKUP_FILE }}" \
            "s3://${R2_BUCKET}/backups/database/${{ env.BACKUP_FILE }}" \
            --endpoint-url "${ENDPOINT}"

          echo "Backup enviado para R2: backups/database/${{ env.BACKUP_FILE }}"

      - name: Limpar backups com mais de 30 dias
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          CUTOFF_DATE=$(date -d '30 days ago' +%Y%m%d)

          echo "Removendo backups anteriores a ${CUTOFF_DATE}..."

          aws s3 ls "s3://${R2_BUCKET}/backups/database/" \
            --endpoint-url "${ENDPOINT}" 2>/dev/null | while read -r line; do
            FILE=$(echo "$line" | awk '{print $4}')
            if [ -z "$FILE" ]; then continue; fi
            FILE_DATE=$(echo "$FILE" | grep -oP '\d{8}' | head -1)
            if [[ -n "$FILE_DATE" && "$FILE_DATE" < "$CUTOFF_DATE" ]]; then
              aws s3 rm "s3://${R2_BUCKET}/backups/database/${FILE}" \
                --endpoint-url "${ENDPOINT}"
              echo "Removido: $FILE"
            fi
          done

          echo "Limpeza concluida"

# =============================================================
# COMO RESTAURAR UM BACKUP:
#
# 1. Baixar o backup do R2:
#    aws s3 cp s3://BUCKET/backups/database/backup_XXXXXXXX_XXXXXX.sql.gz ./backup.sql.gz \
#      --endpoint-url https://ACCOUNT_ID.r2.cloudflarestorage.com
#
# 2. Descompactar:
#    gunzip backup.sql.gz
#
# 3. Restaurar no banco:
#    psql "$DATABASE_URL" < backup.sql
# =============================================================
