name: Backup Database (Diário)

# Executa diariamente às 06:00 UTC (03:00 BRT)
# Também pode ser executado manualmente pelo GitHub
on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:

jobs:
  backup-database:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Instalar cliente PostgreSQL
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y -qq postgresql-client

      - name: Testar conexao com o banco
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          # Extrair hostname da URL para testar DNS
          HOST=$(echo "$DATABASE_URL" | sed -E 's|.*@([^:]+):.*|\1|')
          echo "Testando DNS para: $HOST"
          nslookup "$HOST" || echo "DNS falhou, tentando dig..."
          dig +short "$HOST" || echo "dig tambem falhou"
          echo "Testando conexao com psql..."
          psql "$DATABASE_URL" -c "SELECT 1 AS connection_ok;" || echo "Conexao falhou"

      - name: Criar backup do banco de dados
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          FILENAME="backup_${TIMESTAMP}.sql.gz"

          echo "Iniciando backup do banco de dados..."
          pg_dump "$DATABASE_URL" --no-owner --no-privileges 2>&1 | gzip > "$FILENAME"

          FILE_SIZE=$(du -h "$FILENAME" | cut -f1)
          echo "Backup criado: $FILENAME ($FILE_SIZE)"

          # Verificar se o backup tem conteudo real (mais de 1KB)
          FILE_BYTES=$(stat --format=%s "$FILENAME")
          if [ "$FILE_BYTES" -lt 1000 ]; then
            echo "ERRO: Backup muito pequeno ($FILE_BYTES bytes). Provavelmente falhou."
            echo "Tentando com sslmode=require..."
            pg_dump "${DATABASE_URL}?sslmode=require" --no-owner --no-privileges 2>&1 | gzip > "$FILENAME"
            FILE_SIZE=$(du -h "$FILENAME" | cut -f1)
            FILE_BYTES=$(stat --format=%s "$FILENAME")
            echo "Segunda tentativa: $FILENAME ($FILE_SIZE, $FILE_BYTES bytes)"
            if [ "$FILE_BYTES" -lt 1000 ]; then
              echo "ERRO: Backup ainda falhou. Verifique o DATABASE_URL nos secrets."
              exit 1
            fi
          fi

          echo "BACKUP_FILE=$FILENAME" >> $GITHUB_ENV

      - name: Upload backup para Cloudflare R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          aws s3 cp "${{ env.BACKUP_FILE }}" \
            "s3://${R2_BUCKET}/backups/database/${{ env.BACKUP_FILE }}" \
            --endpoint-url "${ENDPOINT}"

          echo "Backup enviado para R2: backups/database/${{ env.BACKUP_FILE }}"

      - name: Limpar backups com mais de 30 dias
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          CUTOFF_DATE=$(date -d '30 days ago' +%Y%m%d)

          echo "Removendo backups anteriores a ${CUTOFF_DATE}..."

          aws s3 ls "s3://${R2_BUCKET}/backups/database/" \
            --endpoint-url "${ENDPOINT}" 2>/dev/null | while read -r line; do
            FILE=$(echo "$line" | awk '{print $4}')
            if [ -z "$FILE" ]; then continue; fi
            FILE_DATE=$(echo "$FILE" | grep -oP '\d{8}' | head -1)
            if [[ -n "$FILE_DATE" && "$FILE_DATE" < "$CUTOFF_DATE" ]]; then
              aws s3 rm "s3://${R2_BUCKET}/backups/database/${FILE}" \
                --endpoint-url "${ENDPOINT}"
              echo "Removido: $FILE"
            fi
          done

          echo "Limpeza concluida"

# =============================================================
# COMO RESTAURAR UM BACKUP:
#
# 1. Baixar o backup do R2:
#    aws s3 cp s3://BUCKET/backups/database/backup_XXXXXXXX_XXXXXX.sql.gz ./backup.sql.gz \
#      --endpoint-url https://ACCOUNT_ID.r2.cloudflarestorage.com
#
# 2. Descompactar:
#    gunzip backup.sql.gz
#
# 3. Restaurar no banco:
#    psql "$DATABASE_URL" < backup.sql
# =============================================================
