name: Backup Database (Diario)

on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:

jobs:
  backup-database:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Instalar cliente PostgreSQL
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y -qq postgresql-client

      - name: Testar conexao com o banco
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          HOST=$(echo "$DATABASE_URL" | sed -E 's|.*@([^:]+):.*|\1|')
          PORT=$(echo "$DATABASE_URL" | sed -E 's|.*:([0-9]+)/.*|\1|')
          echo "Host: $HOST"
          echo "Porta: $PORT"
          echo "Testando conexao..."
          psql "$DATABASE_URL" -c "SELECT 1 AS connection_ok;" || echo "Conexao falhou"

      - name: Criar backup do banco de dados
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          FILENAME="backup_${TIMESTAMP}.sql"
          FILENAME_GZ="${FILENAME}.gz"

          echo "Iniciando backup..."

          # Tentar pg_dump direto
          pg_dump "$DATABASE_URL" --no-owner --no-privileges -f "$FILENAME" 2>pg_error.log || true

          if [ -s pg_error.log ]; then
            echo "Erros pg_dump:"
            cat pg_error.log
          fi

          # Se falhou ou arquivo vazio, tentar com --no-synchronized-snapshots
          if [ ! -s "$FILENAME" ]; then
            echo "Tentando com --no-synchronized-snapshots..."
            pg_dump "$DATABASE_URL" --no-owner --no-privileges --no-synchronized-snapshots -f "$FILENAME" 2>pg_error2.log || true
            if [ -s pg_error2.log ]; then
              echo "Erros:"
              cat pg_error2.log
            fi
          fi

          # Se ainda falhou, usar psql para dump basico
          if [ ! -s "$FILENAME" ]; then
            echo "pg_dump falhou. Usando psql para backup das tabelas..."
            psql "$DATABASE_URL" -c "SELECT table_name FROM information_schema.tables WHERE table_schema='public';" > "$FILENAME" 2>psql_error.log || true
            if [ -s psql_error.log ]; then
              echo "Erros psql:"
              cat psql_error.log
            fi
          fi

          if [ -s "$FILENAME" ]; then
            gzip "$FILENAME"
            FILE_SIZE=$(du -h "$FILENAME_GZ" | cut -f1)
            FILE_BYTES=$(stat --format=%s "$FILENAME_GZ")
            echo "Backup criado: $FILENAME_GZ ($FILE_SIZE, $FILE_BYTES bytes)"
            echo "BACKUP_FILE=$FILENAME_GZ" >> $GITHUB_ENV
          else
            echo "ERRO: Todas as tentativas falharam."
            exit 1
          fi

      - name: Upload backup para Cloudflare R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          aws s3 cp "${{ env.BACKUP_FILE }}" \
            "s3://${R2_BUCKET}/backups/database/${{ env.BACKUP_FILE }}" \
            --endpoint-url "${ENDPOINT}"

          echo "Backup enviado para R2: backups/database/${{ env.BACKUP_FILE }}"

      - name: Limpar backups com mais de 30 dias
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          CUTOFF_DATE=$(date -d '30 days ago' +%Y%m%d)
          echo "Removendo backups anteriores a ${CUTOFF_DATE}..."

          aws s3 ls "s3://${R2_BUCKET}/backups/database/" \
            --endpoint-url "${ENDPOINT}" 2>/dev/null | while read -r line; do
            FILE=$(echo "$line" | awk '{print $4}')
            if [ -z "$FILE" ]; then continue; fi
            FILE_DATE=$(echo "$FILE" | grep -oP '\d{8}' | head -1)
            if [[ -n "$FILE_DATE" && "$FILE_DATE" < "$CUTOFF_DATE" ]]; then
              aws s3 rm "s3://${R2_BUCKET}/backups/database/${FILE}" \
                --endpoint-url "${ENDPOINT}"
              echo "Removido: $FILE"
            fi
          done
          echo "Limpeza concluida"
