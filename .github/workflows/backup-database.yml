name: Backup Database (Diario)

on:
  schedule:
    - cron: '0 6 * * *'
  workflow_dispatch:

jobs:
  backup-database:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Instalar cliente PostgreSQL 17
        run: |
          sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
          wget -qO- https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo tee /etc/apt/trusted.gpg.d/pgdg.asc
          sudo apt-get update -qq
          sudo apt-get install -y -qq postgresql-client-17
          echo "Versao padrao:"
          pg_dump --version
          echo "Versao 17:"
          /usr/lib/postgresql/17/bin/pg_dump --version

      - name: Testar conexao com o banco
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          HOST=$(echo "$DATABASE_URL" | sed -E 's|.*@([^:]+):.*|\1|')
          PORT=$(echo "$DATABASE_URL" | sed -E 's|.*:([0-9]+)/.*|\1|')
          echo "Host: $HOST"
          echo "Porta: $PORT"
          echo "Testando conexao..."
          psql "$DATABASE_URL" -c "SELECT 1 AS connection_ok;" || echo "Conexao falhou"

      - name: Criar backup do banco de dados
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          FILENAME="backup_${TIMESTAMP}.sql"
          FILENAME_GZ="${FILENAME}.gz"

          echo "Iniciando backup..."

          /usr/lib/postgresql/17/bin/pg_dump "$DATABASE_URL" --no-owner --no-privileges -f "$FILENAME" 2>pg_error.log || true

          if [ -s pg_error.log ]; then
            echo "Avisos/erros do pg_dump:"
            cat pg_error.log
          fi

          if [ -s "$FILENAME" ]; then
            gzip "$FILENAME"
            FILE_SIZE=$(du -h "$FILENAME_GZ" | cut -f1)
            FILE_BYTES=$(stat --format=%s "$FILENAME_GZ")
            echo "Backup criado: $FILENAME_GZ ($FILE_SIZE, $FILE_BYTES bytes)"
            echo "BACKUP_FILE=$FILENAME_GZ" >> $GITHUB_ENV
          else
            echo "ERRO: pg_dump falhou. Verifique os logs acima."
            exit 1
          fi

      - name: Upload backup para Cloudflare R2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"

          aws s3 cp "${{ env.BACKUP_FILE }}" \
            "s3://${R2_BUCKET}/backups/database/${{ env.BACKUP_FILE }}" \
            --endpoint-url "${ENDPOINT}"

          echo "Backup enviado para R2: backups/database/${{ env.BACKUP_FILE }}"

      - name: Limpar backups com mais de 30 dias
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}
          R2_ACCOUNT_ID: ${{ secrets.R2_ACCOUNT_ID }}
          R2_BUCKET: ${{ secrets.R2_BUCKET_NAME }}
        run: |
          ENDPOINT="https://${R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
          CUTOFF_DATE=$(date -d '30 days ago' +%Y%m%d)
          echo "Removendo backups anteriores a ${CUTOFF_DATE}..."

          aws s3 ls "s3://${R2_BUCKET}/backups/database/" \
            --endpoint-url "${ENDPOINT}" 2>/dev/null | while read -r line; do
            FILE=$(echo "$line" | awk '{print $4}')
            if [ -z "$FILE" ]; then continue; fi
            FILE_DATE=$(echo "$FILE" | grep -oP '\d{8}' | head -1)
            if [[ -n "$FILE_DATE" && "$FILE_DATE" < "$CUTOFF_DATE" ]]; then
              aws s3 rm "s3://${R2_BUCKET}/backups/database/${FILE}" \
                --endpoint-url "${ENDPOINT}"
              echo "Removido: $FILE"
            fi
          done
          echo "Limpeza concluida"
